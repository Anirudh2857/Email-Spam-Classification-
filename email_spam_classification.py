# -*- coding: utf-8 -*-
"""Email Spam Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XsLYbfjzwKmL65e1nvy5iW3g5_psTVA8
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns

spam_df = pd.read_csv("emails.csv")

spam_df

spam_df.groupby('spam').describe()

spam_df['length'] = spam_df['text'].apply(len)
spam_df.head()

ham = spam_df[spam_df['spam']==0]

spam = spam_df[spam_df['spam']==1]

ham

spam

spam['length'].plot(bins=60, kind='hist')

ham['length'].plot(bins=60, kind='hist')

import string
import nltk
string.punctuation

nltk.download('stopwords')

from nltk.corpus import stopwords
stopwords.words('english')

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sample_data)

print(vectorizer.get_feature_names())

# Let's define a pipeline to clean up all the messages 
# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation]
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

# Let's test the newly added function
spam_df_clean = spam_df['text'].apply(message_cleaning)

print(spam_df_clean[0])

print(spam_df['text'][0])

#Applying CountVectorizer 
from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning)
spamham_countvectorizer = vectorizer.fit_transform(spam_df['text'])

print(spamham_countvectorizer.toarray())

#Training the model on Naive Bayes 
from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
label = spam_df['spam'].values
NB_classifier.fit(spamham_countvectorizer, label)

#Performing TF-IDF
spamham_countvectorizer

from sklearn.feature_extraction.text import TfidfTransformer

emails_tfidf = TfidfTransformer().fit_transform(spamham_countvectorizer)
print(emails_tfidf.shape)

print(emails_tfidf[:,:])
# Sparse matrix with all the values of IF-IDF

#Dividing the dataset into training and testing 
X = emails_tfidf
y = label

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#Training the model on Naive Bayes 
from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

#Predicting 
from sklearn.metrics import classification_report, confusion_matrix
y_predict_train = NB_classifier.predict(X_train)
y_predict_train
cm = confusion_matrix(y_train, y_predict_train)
sns.heatmap(cm, annot=True)

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

#Descsion Tree 
classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier2.fit(X_train, y_train)

y_predict_test1 = classifier2.predict(X_test)
print(classification_report(y_test, y_predict_test1))

#SVM
classifier3 = SVC(kernel = 'linear', random_state = 0)
classifier3.fit(X_train, y_train)

y_predict_test2 = classifier3.predict(X_test)
print(classification_report(y_test, y_predict_test2))

#KNN
classifier4 = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)
classifier4.fit(X_train, y_train)

y_predict_test3 = classifier4.predict(X_test)
print(classification_report(y_test, y_predict_test3))

#SVM
classifier5 = SVC(kernel = 'linear', random_state = 0)
classifier5.fit(X_train, y_train)

y_predict_test4 = classifier5.predict(X_test)
print(classification_report(y_test, y_predict_test4))

classifier6 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier6.fit(X_train, y_train)

y_predict_test5 = classifier5.predict(X_test)
print(classification_report(y_test, y_predict_test5))

